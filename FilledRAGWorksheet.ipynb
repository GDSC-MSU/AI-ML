{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§  Exploring RAG with a Chatbot That Knows Everything (Almost)!\n",
        "\n",
        "Welcome to the world of **Retrieval-Augmented Generation (RAG)**! Today, weâ€™ll learn how LLMs can get **even smarter** by looking things up. ğŸ“šğŸ’¡  \n",
        "Letâ€™s build a chatbot that can pull in knowledge from outside its brain â€” just like us using Google during a test (but, uh, legally).\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is RAG?\n",
        "\n",
        "RAG stands for **Retrieval-Augmented Generation**. Itâ€™s like giving your chatbot a **superpower** â€” the ability to look things up before answering! ğŸ¦¸â€â™‚ï¸âœ¨\n",
        "\n",
        "Hereâ€™s how it works:\n",
        "1. You ask a question ğŸ¤”  \n",
        "2. The bot searches a library or document ğŸ—ƒï¸  \n",
        "3. It finds the most relevant info ğŸ”  \n",
        "4. Then it gives you a smart answer based on that info ğŸ§ ğŸ’¬\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why Use RAG?\n",
        "\n",
        "ğŸ”’ **LLMs have limits**: They forget stuff, canâ€™t always access the latest info, and sometimes just make things up (weird, right?).\n",
        "\n",
        "âœ… **RAG fixes that**: It lets your bot pull in **real documents**, notes, or web pages â€” whatever you want.\n",
        "\n",
        "**Example:**  \n",
        "Without RAG: â€œWhat are MSUâ€™s dining hours?â€ â†’ *â€œUhâ€¦ not sure.â€*  \n",
        "With RAG: â†’ Searches MSUâ€™s dining website â†’ Gives actual hours!\n",
        "\n",
        "---\n",
        "\n",
        "## 3. What You Need ğŸ§°\n",
        "\n",
        "- ğŸ§  A Language Model (like Gemini, GPT, etc.)\n",
        "- ğŸ“„ A bunch of documents or web content\n",
        "- ğŸ§² A way to search those docs (we use **embeddings** + a **vector database**)\n",
        "- ğŸ§ª A little Python magic (and curiosity)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Letâ€™s Build It! âš™ï¸\n",
        "\n",
        "You may need to clear your variables during testing!"
      ],
      "metadata": {
        "id": "-pXb8R6jVROB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "metadata": {
        "id": "pgdP_lM1CJCa"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 1: Install Your Tools"
      ],
      "metadata": {
        "id": "Vxkg8pi5CIrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "vwDD0_jUGFlc"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
        "!pip install -qU langchain-huggingface\n",
        "!pip install -qU langchain-core\n",
        "!pip install --quiet wikipedia\n",
        "!pip install --quiet google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Set Up the Chatbot\n",
        "Now, let's configure our chatbot to use the Google Gemini API."
      ],
      "metadata": {
        "id": "e7zr0AbsXL_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure the Gemini API client with your API key\n",
        "genai.configure(api_key=\"AIzaSyCYek53buhK0dtiUFsSbVmsKeUXUeC-fFo\")\n",
        "\n",
        "# Define the system instruction to guide the model's response\n",
        "response_system_instruction = (\n",
        "    \"You are an intelligent assistant designed to answer questions using both your general knowledge and additional context provided to you. The context will be inserted into the prompt and may include documents, articles, or notes retrieved from external sources.\"\n",
        "    \"Use the provided context as your primary source of truth when answering. If the context does not contain the necessary information, respond based on your general knowledge, and clearly indicate when you are doing so. Do not fabricate facts.\"\n",
        "    \"Be concise, accurate, and helpful.\"\n",
        ")\n",
        "\n",
        "# Initialize the Gemini model\n",
        "llm = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=response_system_instruction)"
      ],
      "metadata": {
        "id": "QngvjPTNGRwj"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Set Up the Embedding Model\n",
        "\n",
        "We will make use of a free embedding model from Hugging Face."
      ],
      "metadata": {
        "id": "2YNsEGhMYWp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Load the embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "c69SwTYzHkVK"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Load the relevant documents\n",
        "\n",
        "We will create a list of topics we want our model to learn about. The model will store context from Wikipedia's search result and answer questions with the learned context."
      ],
      "metadata": {
        "id": "jNmL49m50xGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Topics to load from Wikipedia\n",
        "TOPICS = ['Einstein', 'Heaviside', 'Faraday']\n",
        "\n",
        "# Load Wikipedia Articles\n",
        "wiki_docs = []\n",
        "for topic in TOPICS:\n",
        "    try:\n",
        "        # content = wikipedia.page(topic).content\n",
        "        content = wikipedia.summary(topic,sentences=50)\n",
        "        doc = Document(page_content=content, metadata={\"source\": f\"Wikipedia: {topic}\"})\n",
        "        wiki_docs.append(doc)\n",
        "        print(f\"âœ… Loaded Wikipedia article: {topic} ({len(content)} characters)\")\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"âš ï¸ Disambiguation error for '{topic}': {e.options[:3]}\")\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"âŒ Page not found for '{topic}'.\")\n",
        "\n",
        "all_docs = wiki_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gphvBV2JSNBL",
        "outputId": "36cfb091-cdbe-4633-f24b-6201f027bdb9"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded Wikipedia article: Einstein (1449 characters)\n",
            "âœ… Loaded Wikipedia article: Heaviside (2171 characters)\n",
            "âœ… Loaded Wikipedia article: Faraday (1460 characters)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Set Up the Text Splitter\n",
        "\n",
        "We need to slice our context into a more practical size. This makes it more token efficient, and also ensures that only the most relevant context is fetched."
      ],
      "metadata": {
        "id": "pTEktq1Kdiwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.split_documents(all_docs)\n",
        "print(f\"ğŸ§© Split into {len(all_splits)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnPpH_mtSY9y",
        "outputId": "29c9b7f1-7676-47f2-875d-0ef8774cc55d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§© Split into 8 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Set Up the Memory Store\n",
        "\n",
        "We will make use of an in-memory store to keep things simple. In practice, you would want to use a database (I prefer Chroma DB for this)."
      ],
      "metadata": {
        "id": "f_MCRKMVYue0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore.from_documents(all_splits, embedding_model)\n",
        "print(f\"âœ… Vector store created with {len(all_docs)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyEklEJQSo6d",
        "outputId": "eef6e0f1-40eb-4c1c-e916-9f0046347801"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vector store created with 3 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Set Up the Retrieval and the Generation\n",
        "\n",
        "These functions will actually handle the retrieval of the context from our data store, and the use of it to assist in generating relevant output. We want to ensure we only retrieve context if it is relevant, so we will throw out irrelevant context."
      ],
      "metadata": {
        "id": "lOIctUhtA9WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieval(user_input: str, threshold: float = 0.5) -> tuple[str, list]:\n",
        "    try:\n",
        "        # Try to get documents with similarity scores (if supported)\n",
        "        results = vector_store.similarity_search_with_score(user_input, k=4)\n",
        "\n",
        "        # Filter by relevance threshold\n",
        "        filtered_docs = [doc for doc, score in results if score >= threshold]\n",
        "\n",
        "    except AttributeError:\n",
        "        # If .similarity_search_with_score() is not supported, fallback\n",
        "        filtered_docs = vector_store.similarity_search(user_input, k=4)\n",
        "\n",
        "    if not filtered_docs:\n",
        "        return \"No relevant documents found.\", []\n",
        "\n",
        "    # Format context with source metadata\n",
        "    context_chunks = []\n",
        "    for i, doc in enumerate(filtered_docs, start=1):\n",
        "        source = doc.metadata.get(\"source\", f\"Document {i}\")\n",
        "        context_chunks.append(f\"[{source}]\\n{doc.page_content.strip()}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    return context, filtered_docs\n",
        "\n",
        "\n",
        "def generation(user_input: str, context: str) -> str:\n",
        "    # Invoke the LangChain Hub prompt (it will use the context + question)\n",
        "    prompt_value = prompt.invoke({\n",
        "        \"context\": context,\n",
        "        \"question\": f\"{user_input} \\n\\nPlease cite sources clearly in your answer.\"\n",
        "    })\n",
        "    full_prompt = prompt_value.to_string()\n",
        "\n",
        "    # Generate response using Gemini\n",
        "    response = llm.generate_content(full_prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "S41749NbA84I"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8:\n",
        "\n",
        "Test out your RAG chatbot!"
      ],
      "metadata": {
        "id": "SKdLuUqxEwzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# Load RAG prompt from LangChain Hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "print(\"ğŸ’¬ Ask me anything! (type 'exit' to quit)\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"ğŸ‘‹ Goodbye!\")\n",
        "        break\n",
        "\n",
        "    context, docs = retrieval(user_input)\n",
        "    response = generation(user_input, context)\n",
        "\n",
        "    # Print response\n",
        "    print(f\"\\nğŸ¤–: {response.text.strip()}\\n\")\n",
        "\n",
        "\n",
        "    # Print source list\n",
        "    print(\"\\nğŸ“š Sources Used:\")\n",
        "    for i, doc in enumerate(docs, start=1):\n",
        "        print(f\"{i}. {doc.metadata.get('source', 'Unknown')}\")\n"
      ],
      "metadata": {
        "id": "WV9ESentS_zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ² Section 5: Fun Activity â€“ RAG Roleplay! ğŸ­ğŸ§ \n",
        "\n",
        "Letâ€™s play RAG in real life! You and a partner will act out the two roles in a Retrieval-Augmented Generation system:\n",
        "\n",
        "### ğŸ‘¤ Person A: The Retriever\n",
        "Youâ€™re the vector store. You hold the documents (Wikipedia, notes, etc.).  \n",
        "When Person B asks a question, you:\n",
        "1. Search your â€œmental documentsâ€\n",
        "2. Choose 2â€“3 facts that best answer the question\n",
        "3. Share those facts as â€œcontextâ€\n",
        "\n",
        "### ğŸ¤– Person B: The Generator (a chatbot)\n",
        "You get only what the retriever gives you â€” no peeking at outside info!\n",
        "Use the context to answer the question clearly in 2â€“3 sentences.\n",
        "\n",
        "---\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- **Question:** What is the Doppler Effect?\n",
        "- **Context from A:**  \n",
        "  1. The Doppler Effect explains the change in frequency of waves based on motion.  \n",
        "  2. It happens with sound, light, or other waves when the source or observer moves.  \n",
        "  3. Itâ€™s why ambulance sirens change pitch as they pass by.\n",
        "\n",
        "- **Answer from B:**  \n",
        "  The Doppler Effect is the change in frequency of waves when the source or observer moves. Thatâ€™s why a siren sounds different as it gets closer and then farther away.\n",
        "\n",
        "---\n",
        "\n",
        "Now switch roles and try with new questions! âœ¨  \n",
        "Try: *What causes seasons?* or *Who was Nikola Tesla?*"
      ],
      "metadata": {
        "id": "ryfNg0Yt72Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ‰ Section 6: Wrapping Up â€“ You Built a Smarter Bot!\n",
        "\n",
        "Amazing work! Today, you explored how to give a chatbot superpowers using Retrieval-Augmented Generation (RAG).  \n",
        "Letâ€™s reflect on what youâ€™ve accomplished:\n",
        "\n",
        "âœ… You learned how LLMs generate answers  \n",
        "âœ… You explored their limitations and how RAG helps  \n",
        "âœ… You loaded real documents into a vector store  \n",
        "âœ… You retrieved relevant context and generated responses  \n",
        "âœ… You even roleplayed a mini RAG system yourself!\n",
        "\n",
        "---\n",
        "\n",
        "Now youâ€™re ready to:\n",
        "\n",
        "- Build a bot that reads your study notes ğŸ§   \n",
        "- Create a travel helper that reads blogs âœˆï¸  \n",
        "- Build a chatbot for your school or club ğŸ«  \n",
        "\n",
        "The possibilities are endless when you give your chatbot access to the right info.\n",
        "\n",
        "ğŸ› ï¸ Try customizing:\n",
        "- What documents it uses\n",
        "- How it responds (serious, funny, formal)\n",
        "- What itâ€™s an expert in\n",
        "\n",
        "ğŸ’¡ Bonus idea: Combine RAG with a voice assistant or GUI using Streamlit or Gradio!\n",
        "\n",
        "ğŸš€ Go build something cool â€” and don't forget to share it!\n"
      ],
      "metadata": {
        "id": "gW5DVPm_Fi80"
      }
    }
  ]
}